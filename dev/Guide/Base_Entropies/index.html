<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Base Entropies · EntropyHub.jl</title><link rel="canonical" href="https://MattWillFlood.github.io/EntropyHub.jl/Guide\\Base_Entropies/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="EntropyHub.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">EntropyHub.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Base Entropies</a></li><li><a class="tocitem" href="../Cross_Entropies/">Cross-Entropies</a></li><li><a class="tocitem" href="../Multiscale_Entropies/">Multiscale Entropies</a></li><li><a class="tocitem" href="../Multiscale_Cross_Entropies/">Multiscale Cross-Entropies</a></li><li><a class="tocitem" href="../Bidimensional_Entropies/">Bidimensional Entropies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Examples/Examples/">Notes on Examples</a></li><li><a class="tocitem" href="../../Examples/Example1/">Ex.1: Sample Entropy</a></li><li><a class="tocitem" href="../../Examples/Example2/">Ex.2: Permutation Entropy</a></li><li><a class="tocitem" href="../../Examples/Example3/">Ex.3: Phase Entropy</a></li><li><a class="tocitem" href="../../Examples/Example4/">Ex.4: Cross-Distribution Entropy</a></li><li><a class="tocitem" href="../../Examples/Example5/">Ex.5: Multiscale Entropy Object</a></li><li><a class="tocitem" href="../../Examples/Example6/">Ex.6: Multiscale [Increment] Entropy</a></li><li><a class="tocitem" href="../../Examples/Example7/">Ex.7: Refined Multiscale [Sample] Entropy</a></li><li><a class="tocitem" href="../../Examples/Example8/">Ex.8: Composite Multiscale Cross-Approximate Entropy</a></li><li><a class="tocitem" href="../../Examples/Example9/">Ex.9: Hierarchical Multiscale corrected Cross-Conditional Entropy</a></li><li><a class="tocitem" href="../../Examples/Example10/">Ex.10: Bidimensional Fuzzy Entropy</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guide</a></li><li class="is-active"><a href>Base Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Base Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/master/docs/src/Guide/Base_Entropies.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Base-Entropies"><a class="docs-heading-anchor" href="#Base-Entropies">Base Entropies</a><a id="Base-Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Base-Entropies" title="Permalink"></a></h1><p><strong><em>Functions for estimating the entropy of a single univariate time series.</em></strong></p><p>The following functions also form the base entropy method used by <a href="Guide/Multiscale_Entropies.html">Multiscale functions</a>.</p><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._ApEn.ApEn" href="#EntropyHub._ApEn.ApEn"><code>EntropyHub._ApEn.ApEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Ap, Phi = ApEn(Sig)</code></pre><p>Returns the approximate entropy estimates <code>Ap</code> and the log-average number of  matched vectors <code>Phi</code> for <code>m</code> = [0,1,2], estimated from the data sequence <code>Sig</code> using the default parameters: embedding dimension = 2, time delay = 1, radius distance threshold = 0.2*SD(<code>Sig</code>), logarithm = natural</p><pre><code class="language-none">Ap, Phi = ApEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, r::Real=0.2*std(Sig,corrected=false), Logx::Real=exp(1))</code></pre><p>Returns the approximate entropy estimates <code>Ap</code> of the data sequence <code>Sig</code> for dimensions = [0,1,...,<code>m</code>] using the specified keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>      - Embedding Dimension, a positive integer</p><p><code>tau</code>    - Time Delay, a positive integer</p><p><code>r</code>      - Radius Distance Threshold, a positive scalar  </p><p><code>Logx</code>   - Logarithm base, a positive scalar</p><p><strong>See also <code>XApEn</code>, <code>SampEn</code>, <code>MSEn</code>, <code>FuzzEn</code>, <code>PermEn</code>, <code>CondEn</code>, <code>DispEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Steven M. Pincus, 
    &quot;Approximate entropy as a measure of system complexity.&quot; 
    Proceedings of the National Academy of Sciences 
    88.6 (1991): 2297-2301.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_ApEn.jl#L4-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._SampEn.SampEn" href="#EntropyHub._SampEn.SampEn"><code>EntropyHub._SampEn.SampEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Samp, A, B = SampEn(Sig)</code></pre><p>Returns the sample entropy estimates <code>Samp</code> and the number of matched state   vectors (<code>m</code>:B, <code>m+1</code>:A) for <code>m</code> = [0,1,2] estimated from the data sequence <code>Sig</code>  using the default parameters: embedding dimension = 2, time delay = 1,   radius threshold = 0.2*SD(<code>Sig</code>), logarithm = natural</p><pre><code class="language-none">Samp, A, B = SampEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, r::Real=0.2*std(Sig,corrected=false), Logx::Real=exp(1))</code></pre><p>Returns the sample entropy estimates <code>Samp</code> for dimensions = [0,1,...,<code>m</code>]  estimated from the data sequence <code>Sig</code> using the specified keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, a positive integer</p><p><code>tau</code>   - Time Delay, a positive integer</p><p><code>r</code>     - Radius Distance Threshold, a positive scalar  </p><p><code>Logx</code>  - Logarithm base, a positive scalar </p><p><strong>See also <code>ApEn</code>, <code>FuzzEn</code>, <code>PermEn</code>, <code>CondEn</code>, <code>XSampEn</code>, <code>SampEn2D</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Joshua S Richman and J. Randall Moorman. 
    &quot;Physiological time-series analysis using approximate entropy
    and sample entropy.&quot; 
    American Journal of Physiology-Heart and Circulatory Physiology (2000).</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_SampEn.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._FuzzEn.FuzzEn" href="#EntropyHub._FuzzEn.FuzzEn"><code>EntropyHub._FuzzEn.FuzzEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Fuzz, Ps1, Ps2 = FuzzEn(Sig)</code></pre><p>Returns the fuzzy entropy estimates <code>Fuzz</code> and the average fuzzy distances (<code>m</code>:Ps1, <code>m+1</code>:Ps2) for <code>m</code> = [1,2] estimated from the data sequence <code>Sig</code> using the default parameters: embedding dimension = 2, time delay = 1,  fuzzy function (<code>Fx</code>) = &quot;default&quot;, fuzzy function parameters (<code>r</code>) = [0.2, 2], logarithm = natural</p><pre><code class="language-none">Fuzz, Ps1, Ps2 = FuzzEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, r::Union{Real,Tuple{Real,Real}}=(.2,2), Fx::String=&quot;default&quot;, Logx::Real=exp(1))</code></pre><p>Returns the fuzzy entropy estimates <code>Fuzz</code> for dimensions = [1,...,<code>m</code>] estimated for the data sequence <code>Sig</code> using the specified keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, a positive integer  [default: 2]</p><p><code>tau</code>   - Time Delay, a positive integer        [default: 1]</p><p><code>Fx</code>    - Fuzzy function name, one of the following:            {<code>&quot;sigmoid&quot;, &quot;modsampen&quot;, &quot;default&quot;, &quot;gudermannian&quot;, &quot;linear&quot;</code>}</p><p><code>r</code>     - Fuzzy function parameters, a 1 element scalar or a 2 element           tuple of positive values. The <code>r</code> parameters for each fuzzy           function are defined as follows:      [default: [.2 2]]</p><pre><code class="language-none">        sigmoid:      r(1) = divisor of the exponential argument
                      r(2) = value subtracted from argument (pre-division)
        modsampen:    r(1) = divisor of the exponential argument
                      r(2) = value subtracted from argument (pre-division)
        default:      r(1) = divisor of the exponential argument
                      r(2) = argument exponent (pre-division)
        gudermannian: r  = a scalar whose value is the numerator of
                            argument to gudermannian function:
                            GD(x) = atan(tanh(r/x))
        linear:       r  = an integer value. When r = 0, the
                            argument of the exponential function is 
                            normalised between [0 1]. When r = 1,
                            the minimuum value of the exponential 
                            argument is set to 0.</code></pre><p><code>Logx</code>  - Logarithm base, a positive scalar  [default: natural]</p><p><strong>For further information on keyword arguments, see the EntropyHub guide.</strong></p><p><strong>See also <code>SampEn</code>, <code>ApEn</code>, <code>PermEn</code>, <code>DispEn</code>, <code>XFuzzEn</code>, <code>FuzzEn2D</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Weiting Chen, et al.
    &quot;Characterization of surface EMG signal based on fuzzy entropy.&quot;
    IEEE Transactions on neural systems and rehabilitation engineering
    15.2 (2007): 266-272.

[2] Hong-Bo Xie, Wei-Xing He, and Hui Liu
    &quot;Measuring time series regularity using nonlinear
    similarity-based sample entropy.&quot;
    Physics Letters A
    372.48 (2008): 7140-7146.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_FuzzEn.jl#L5-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._K2En.K2En" href="#EntropyHub._K2En.K2En"><code>EntropyHub._K2En.K2En</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">K2, Ci = K2En(Sig)</code></pre><p>Returns the Kolmogorov entropy estimates <code>K2</code> and the correlation integrals <code>Ci</code> for <code>m</code> = [1,2] estimated from the data sequence <code>Sig</code> using the default parameters: embedding dimension = 2, time delay = 1,  r = 0.2*SD(<code>Sig</code>), logarithm = natural</p><pre><code class="language-none">K2, Ci = K2En(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, r::Real=0.2*std(Sig,corrected=false), Logx::Real=exp(1))</code></pre><p>Returns the Kolmogorov entropy estimates <code>K2</code> for dimensions = [1,...,<code>m</code>] estimated from the data sequence <code>Sig</code> using the &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, a positive integer</p><p><code>tau</code>   - Time Delay, a positive integer</p><p><code>r</code>     - Radius, a positive scalar </p><p><code>Logx</code>  - Logarithm base, a positive scalar</p><p><strong>See also <code>DistEn</code>, <code>XK2En</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Peter Grassberger and Itamar Procaccia,
    &quot;Estimation of the Kolmogorov entropy from a chaotic signal.&quot; 
    Physical review A 28.4 (1983): 2591.

[2] Lin Gao, Jue Wang  and Longwei Chen
    &quot;Event-related desynchronization and synchronization 
    quantification in motor-related EEG by Kolmogorov entropy&quot;
    J Neural Eng. 2013 Jun;10(3):03602</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_K2En.jl#L4-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._PermEn.PermEn" href="#EntropyHub._PermEn.PermEn"><code>EntropyHub._PermEn.PermEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Perm, Pnorm, cPE = PermEn(Sig)</code></pre><p>Returns the permuation entropy estimates <code>Perm</code>, the normalised permutation entropy <code>Pnorm</code> and the conditional permutation entropy <code>cPE</code> for <code>m</code> = [1,2] estimated from the data sequence <code>Sig</code>  using the default  parameters: embedding dimension = 2, time delay = 1, logarithm = base 2,  normalisation = w.r.t #symbols (<code>m</code>-1) Note: using the standard PermEn estimation, <code>Perm</code> = 0 when <code>m</code> = 1.</p><pre><code class="language-none">Perm, Pnorm, cPE = PermEn(Sig, m)</code></pre><p>Returns the permutation entropy estimates <code>Perm</code> estimated from the data sequence <code>Sig</code> using the specified embedding dimensions = [1,...,<code>m</code>]  with other default parameters as listed above.</p><pre><code class="language-none">Perm, Pnorm, cPE = PermEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, Typex::String=&quot;none&quot;, tpx::Union{Real,Nothing}=nothing, Logx::Real=2, Norm::Bool=false)</code></pre><p>Returns the permutation entropy estimates <code>Perm</code> for dimensions = [1,...,<code>m</code>] estimated from the data sequence <code>Sig</code> using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, an integer &gt; 1 </p><p><code>tau</code>   - Time Delay, a positive integer</p><p><code>Logx</code>  - Logarithm base, a positive scalar (enter 0 for natural log) </p><p><code>Norm</code>  - Normalisation of PermEn value:</p><pre><code class="language-none">      false -  normalises w.r.t log(# of permutation symbols [m-1]) - default
      true  -  normalises w.r.t log(# of all possible permutations [m!])
      * Note: Normalised permutation entropy is undefined for m = 1.
      ** Note: When Typex = &#39;uniquant&#39; and Norm = true, normalisation
      is calculated w.r.t. log(tpx^m)</code></pre><p><code>Typex</code>  - Permutation entropy variation, one of the following:           {`&quot;none&quot;, &quot;uniquant&quot;, &quot;finegrain&quot;, &quot;modified&quot;, &quot;ampaware&quot;,           &quot;weighted&quot;, &quot;edge&quot;}           See the EntropyHub guide for more info on PermEn variations.    </p><p><code>tpx</code>   - Tuning parameter for associated permutation entropy variation.</p><pre><code class="language-none">      [uniquant]  &#39;tpx&#39; is the L parameter, an integer &gt; 1 (default = 4).           
      [finegrain] &#39;tpx&#39; is the alpha parameter, a positive scalar (default = 1)
      [ampaware]  &#39;tpx&#39; is the A parameter, a value in range [0 1] (default = 0.5)
      [edge]      &#39;tpx&#39; is the r sensitivity parameter, a scalar &gt; 0 (default = 1)
      See the EntropyHub guide for more info on PermEn variations.</code></pre><p><strong>See also <code>XPermEn</code>, <code>MSEn</code>, <code>XMSEn</code>, <code>SampEn</code>, <code>ApEn</code>, <code>CondEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Christoph Bandt and Bernd Pompe, 
        &quot;Permutation entropy: A natural complexity measure for time series.&quot; 
        Physical Review Letters,
        88.17 (2002): 174102.

[2] Xiao-Feng Liu, and Wang Yue,
        &quot;Fine-grained permutation entropy as a measure of natural 
        complexity for time series.&quot; 
        Chinese Physics B 
        18.7 (2009): 2690.

[3] Chunhua Bian, et al.,
        &quot;Modified permutation-entropy analysis of heartbeat dynamics.&quot;
        Physical Review E
        85.2 (2012) : 021906

[4] Bilal Fadlallah, et al.,
        &quot;Weighted-permutation entropy: A complexity measure for time 
        series incorporating amplitude information.&quot; 
        Physical Review E 
        87.2 (2013): 022911.

[5] Hamed Azami and Javier Escudero,
        &quot;Amplitude-aware permutation entropy: Illustration in spike 
        detection and signal segmentation.&quot; 
        Computer methods and programs in biomedicine,
        128 (2016): 40-51.

[6] Zhiqiang Huo, et al.,
        &quot;Edge Permutation Entropy: An Improved Entropy Measure for 
        Time-Series Analysis,&quot; 
        45th Annual Conference of the IEEE Industrial Electronics Soc,
        (2019), 5998-6003

[8] Zhe Chen, et al. 
        &quot;Improved permutation entropy for measuring complexity of time
        series under noisy condition.&quot; 
        Complexity 
        1403829 (2019).

[9] Maik Riedl, Andreas Müller, and Niels Wessel,
        &quot;Practical considerations of permutation entropy.&quot; 
        The European Physical Journal Special Topics 
        222.2 (2013): 249-262.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_PermEn.jl#L5-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._CondEn.CondEn" href="#EntropyHub._CondEn.CondEn"><code>EntropyHub._CondEn.CondEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Cond, SEw, SEz = CondEn(Sig)</code></pre><p>Returns the corrected conditional entropy estimates (<code>Cond</code>) and the corresponding Shannon entropies (m: <code>SEw</code>, m+1: <code>SEz</code>) for m = [1,2]  estimated from the data sequence (<code>Sig</code>) using the default  parameters: embedding dimension = 2, time delay = 1, symbols = 6, logarithm = natural, normalisation = false <em>Note: CondEn(m=1) returns the Shannon entropy of <code>Sig</code>.</em></p><pre><code class="language-none">Cond, SEw, SEz = CondEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, c::Int=6, Logx::Real=exp(1), Norm::Bool=false)</code></pre><p>Returns the corrected conditional entropy estimates (<code>Cond</code>) from the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, an integer &gt; 1  </p><p><code>tau</code>   - Time Delay, a positive integer  </p><p><code>c</code>     - # of symbols, an integer &gt; 1  </p><p><code>Logx</code>  - Logarithm base, a positive scalar  </p><p><code>Norm</code>  - Normalisation of CondEn value:  </p><pre><code class="language-none">      [false]  no normalisation - default
      [true]   normalises w.r.t Shannon entropy of data sequence `Sig`</code></pre><p><strong>See also <code>XCondEn</code>, <code>MSEn</code>, <code>PermEn</code>, <code>DistEn</code>, <code>XPermEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Alberto Porta, et al.,
    &quot;Measuring regularity by means of a corrected conditional
    entropy in sympathetic outflow.&quot; 
    Biological cybernetics 
    78.1 (1998): 71-78.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_CondEn.jl#L5-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._DistEn.DistEn" href="#EntropyHub._DistEn.DistEn"><code>EntropyHub._DistEn.DistEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Dist, Ppi = DistEn(Sig)</code></pre><p>Returns the distribution entropy estimate (<code>Dist</code>) and the corresponding distribution probabilities (<code>Ppi</code>) estimated from  the data sequence (<code>Sig</code>) using the default  parameters:  embedding dimension = 2, time delay = 1, binning method = &#39;Sturges&#39;, logarithm = base 2, normalisation = w.r.t # of histogram bins</p><pre><code class="language-none">Dist, Ppi = DistEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, Bins::Union{Int,String}=&quot;Sturges&quot;, Logx::Real=2, Norm::Bool=true)</code></pre><p>Returns the distribution entropy estimate (<code>Dist</code>) estimated from  the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, a positive integer  </p><p><code>tau</code>   - Time Delay, a positive integer  </p><p><code>Bins</code>  - Histogram bin selection method for distance distribution,            one of the following:  </p><pre><code class="language-none">      an integer &gt; 1 indicating the number of bins, or one of the 
      following strings {&#39;sturges&#39;,&#39;sqrt&#39;,&#39;rice&#39;,&#39;doanes&#39;}
      [default: &#39;sturges&#39;]</code></pre><p><code>Logx</code>  - Logarithm base, a positive scalar (enter 0 for natural log)   </p><p><code>Norm</code>  - Normalisation of DistEn value:  </p><pre><code class="language-none">      [false]  no normalisation.
      [true]   normalises w.r.t # of histogram bins - default</code></pre><p><strong>See also <code>XDistEn</code>, <code>DistEn2D</code>, <code>MSEn</code>, <code>K2En</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Li, Peng, et al.,
    &quot;Assessing the complexity of short-term heartbeat interval 
    series by distribution entropy.&quot; 
    Medical &amp; biological engineering &amp; computing 
    53.1 (2015): 77-87.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_DistEn.jl#L4-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._SpecEn.SpecEn" href="#EntropyHub._SpecEn.SpecEn"><code>EntropyHub._SpecEn.SpecEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Spec, BandEn = SpecEn(Sig)</code></pre><p>Returns the spectral entropy estimate of the full spectrum (<code>Spec</code>) and the within-band entropy (<code>BandEn</code>) estimated from the data sequence (<code>Sig</code>) using the default  parameters:  N-point FFT = 2*len(<code>Sig</code>) + 1, normalised band edge frequencies = [0 1], logarithm = base 2, normalisation = w.r.t # of spectrum/band frequency values.</p><pre><code class="language-none">Spec, BandEn = SpecEn(Sig::AbstractArray{T,1} where T&lt;:Real; N::Int=1 + (2*size(Sig,1)), Freqs::Tuple{Real,Real}=(0,1), Logx::Real=exp(1), Norm::Bool=true)</code></pre><p>Returns the spectral entropy (<code>Spec</code>) and the within-band entropy (<code>BandEn</code>) estimate for the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>N</code>     - Resolution of spectrum (N-point FFT), an integer &gt; 1  </p><p><code>Freqs</code> - Normalised band edge frequencies, a 2 element tuple with values </p><pre><code class="language-none">      in range [0 1] where 1 corresponds to the Nyquist frequency (Fs/2).
      Note: When no band frequencies are entered, BandEn == SpecEn</code></pre><p><code>Logx</code>  - Logarithm base, a positive scalar (enter 0 for natural log) </p><p><code>Norm</code>  - Normalisation of <code>Spec</code> value:</p><pre><code class="language-none">      [false]  no normalisation.
      [true]   normalises w.r.t # of spectrum/band frequency values - default.</code></pre><p>For more info, see the EntropyHub guide.</p><p><strong>See also <code>XSpecEn</code>, <code>fft</code>, <code>MSEn</code>,  <code>XMSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] G.E. Powell and I.C. Percival,
    &quot;A spectral entropy method for distinguishing regular and 
    irregular motion of Hamiltonian systems.&quot; 
    Journal of Physics A: Mathematical and General 
    12.11 (1979): 2053.

[2] Tsuyoshi Inouye, et al.,
    &quot;Quantification of EEG irregularity by use of the entropy of 
    the power spectrum.&quot; 
    Electroencephalography and clinical neurophysiology 
    79.3 (1991): 204-210.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_SpecEn.jl#L5-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._DispEn.DispEn" href="#EntropyHub._DispEn.DispEn"><code>EntropyHub._DispEn.DispEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Dispx, RDE = DispEn(Sig)</code></pre><p>Returns the dispersion entropy (<code>Dispx</code>) and the reverse dispersion entropy (<code>RDE</code>) estimated from the data sequence (<code>Sig</code>) using the default parameters: embedding dimension = 2, time delay = 1, symbols = 3, logarithm = natural, data transform = normalised cumulative density function (ncdf)</p><pre><code class="language-none">Dispx, RDE = DispEn(Sig::AbstractArray{T,1} where T&lt;:Real; c::Int=3, m::Int=2, tau::Int=1, Typex::String=&quot;ncdf&quot;, Logx::Real=exp(1), Fluct::Bool=false, Norm::Bool=false, rho::Real=1)</code></pre><p>Returns the dispersion entropy (<code>Dispx</code>) and the reverse dispersion entropy (<code>RDE</code>) estimated from the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, a positive integer</p><p><code>tau</code>   - Time Delay, a positive integer</p><p><code>c</code>     - Number of symbols, an integer &gt; 1</p><p><code>Typex</code> - Type of data-to-symbolic sequence transform, one of the following:           {<code>&quot;linear&quot;, &quot;kmeans&quot; ,&quot;ncdf&quot;, &quot;finesort&quot;, &quot;equal&quot;</code>}</p><pre><code class="language-none">      See the EntropyHub guide for more info on these transforms.</code></pre><p><code>Logx</code>  - Logarithm base, a positive scalar</p><p><code>Fluct</code> - When Fluct == true, DispEn returns the fluctuation-based           Dispersion entropy.   [default: false]</p><p><code>Norm</code>  - Normalisation of Dispx value:           [false]  no normalisation - default           [true]   normalises w.r.t # possible vector permutations (c^m).</p><p><code>rho</code>   - <em>If Typex == &#39;finesort&#39;, rho is the tuning parameter</em> (default: 1)</p><p><strong>See also <code>PermEn</code>, <code>SyDyEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Mostafa Rostaghi and Hamed Azami,
    &quot;Dispersion entropy: A measure for time-series analysis.&quot; 
    IEEE Signal Processing Letters 
    23.5 (2016): 610-614.

[2] Hamed Azami and Javier Escudero,
    &quot;Amplitude-and fluctuation-based dispersion entropy.&quot; 
    Entropy 
    20.3 (2018): 210.

[3] Li Yuxing, Xiang Gao and Long Wang,
    &quot;Reverse dispersion entropy: A new complexity measure for 
    sensor signal.&quot; 
    Sensors 
    19.23 (2019): 5203.

[4] Wenlong Fu, et al.,
    &quot;Fault diagnosis for rolling bearings based on fine-sorted 
    dispersion entropy and SVM optimized with mutation SCA-PSO.&quot;
    Entropy
    21.4 (2019): 404.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_DispEn.jl#L6-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._SyDyEn.SyDyEn" href="#EntropyHub._SyDyEn.SyDyEn"><code>EntropyHub._SyDyEn.SyDyEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">SyDy, Zt = SyDyEn(Sig)</code></pre><p>Returns the symbolic dynamic entropy (<code>SyDy</code>) and the symbolic sequence (<code>Zt</code>) of the data sequence (<code>Sig</code>) using the default parameters:  embedding dimension = 2, time delay = 1, symbols = 3, logarithm = natural, symbolic partition type = maximum entropy partitioning (<code>MEP</code>),  normalisation = normalises w.r.t # possible vector permutations (c^m) </p><pre><code class="language-none">SyDy, Zt = SyDyEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, c::Int=3, Typex::String=&quot;MEP&quot;, Logx::Real=exp(1), Norm::Bool=true)</code></pre><p>Returns the symbolic dynamic entropy (<code>SyDy</code>) and the symbolic sequence (<code>Zt</code>) of the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, a positive integer  </p><p><code>tau</code>   - Time Delay, a positive integer  </p><p><code>c</code>     - Number of symbols, an integer &gt; 1  </p><p><code>Typex</code> - Type of symbolic sequnce partitioning method, one of the following:  </p><pre><code class="language-none">      {&quot;linear&quot;,&quot;uniform&quot;,&quot;MEP&quot;(default),&quot;kmeans&quot;}</code></pre><p><code>Logx</code>  - Logarithm base, a positive scalar    </p><p><code>Norm</code>  - Normalisation of SyDyEn value: </p><pre><code class="language-none">      [false]  no normalisation 
      [true]   normalises w.r.t # possible vector permutations (c^m+1) - default</code></pre><p>See the EntropyHub guide for more info on these parameters.</p><p><strong>See also <code>DispEn</code>, <code>PermEn</code>, <code>CondEn</code>, <code>SampEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Yongbo Li, et al.,
    &quot;A fault diagnosis scheme for planetary gearboxes using 
    modified multi-scale symbolic dynamic entropy and mRMR feature 
    selection.&quot; 
    Mechanical Systems and Signal Processing 
    91 (2017): 295-312. 

[2] Jian Wang, et al.,
    &quot;Fault feature extraction for multiple electrical faults of 
    aviation electro-mechanical actuator based on symbolic dynamics
    entropy.&quot; 
    IEEE International Conference on Signal Processing, 
    Communications and Computing (ICSPCC), 2015.

[3] Venkatesh Rajagopalan and Asok Ray,
    &quot;Symbolic time series analysis via wavelet-based partitioning.&quot;
    Signal processing 
    86.11 (2006): 3309-3320.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_SyDyEn.jl#L6-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._IncrEn.IncrEn" href="#EntropyHub._IncrEn.IncrEn"><code>EntropyHub._IncrEn.IncrEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Incr = IncrEn(Sig)</code></pre><p>Returns the increment entropy (<code>Incr</code>) estimate of the data sequence  (<code>Sig</code>) using the default parameters:  embedding dimension = 2, time delay = 1, quantifying resolution = 4, logarithm = base 2,</p><pre><code class="language-none">Incr = IncrEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, R::Int=4, Logx::Real=2, Norm::Bool=false)</code></pre><p>Returns the increment entropy (<code>Incr</code>) estimate of the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, an integer &gt; 1   </p><p><code>tau</code>   - Time Delay, a positive integer    </p><p><code>R</code>     - Quantifying resolution, a positive scalar    </p><p><code>Logx</code>  - Logarithm base, a positive scalar (enter 0 for natural log) </p><p><code>Norm</code>  - Normalisation of IncrEn value: </p><pre><code class="language-none">      [false]  no normalisation - default
      [true]   normalises w.r.t embedding dimension (m-1).</code></pre><p><strong>See also <code>PermEn</code>, <code>SyDyEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Xiaofeng Liu, et al.,
    &quot;Increment entropy as a measure of complexity for time series.&quot;
    Entropy
    18.1 (2016): 22.1.

***   &quot;Correction on Liu, X.; Jiang, A.; Xu, N.; Xue, J. - Increment 
    Entropy as a Measure of Complexity for Time Series,
    Entropy 2016, 18, 22.&quot; 
    Entropy 
    18.4 (2016): 133.

[2] Xiaofeng Liu, et al.,
    &quot;Appropriate use of the increment entropy for 
    electrophysiological time series.&quot; 
    Computers in biology and medicine 
    95 (2018): 13-23.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_IncrEn.jl#L4-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._CoSiEn.CoSiEn" href="#EntropyHub._CoSiEn.CoSiEn"><code>EntropyHub._CoSiEn.CoSiEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">CoSi, Bm = CoSiEn(Sig)</code></pre><p>Returns the cosine similarity entropy (<code>CoSi</code>) and the corresponding global probabilities estimated from the data sequence (<code>Sig</code>) using the default parameters:   embedding dimension = 2, time delay = 1,  angular threshold = .1,  logarithm = base 2,</p><pre><code class="language-none">CoSi, Bm = CoSiEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, r::Real=.1, Logx::Real=2, Norm::Int=0)</code></pre><p>Returns the cosine similarity entropy (<code>CoSi</code>) estimated from the data sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, an integer &gt; 1   </p><p><code>tau</code>   - Time Delay, a positive integer    </p><p><code>r</code>     - Angular threshold, a value in range [0 &lt; r &lt; 1]   </p><p><code>Logx</code>  - Logarithm base, a positive scalar (enter 0 for natural log) </p><p><code>Norm</code>  - Normalisation of <code>Sig</code>, one of the following integers:    </p><pre><code class="language-none">    [0]  no normalisation - default
    [1]  normalises `Sig` by removing median(`Sig`)
    [2]  normalises `Sig` by removing mean(`Sig`)
    [3]  normalises `Sig` w.r.t. SD(`Sig`)
    [4]  normalises `Sig` values to range [-1 1]</code></pre><p><strong>See also <code>PhasEn</code>, <code>SlopEn</code>, <code>GridEn</code>, <code>MSEn</code>, <code>cMSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Theerasak Chanwimalueang and Danilo Mandic,
    &quot;Cosine similarity entropy: Self-correlation-based complexity
    analysis of dynamical systems.&quot;
    Entropy 
    19.12 (2017): 652.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_CoSiEn.jl#L5-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._PhasEn.PhasEn" href="#EntropyHub._PhasEn.PhasEn"><code>EntropyHub._PhasEn.PhasEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Phas = PhasEn(Sig)</code></pre><p>Returns the phase entropy (<code>Phas</code>) estimate of the data sequence (<code>Sig</code>) using the default parameters:  angular partitions = 4, time delay = 1, logarithm = natural,</p><pre><code class="language-none">Phas = PhasEn(Sig::AbstractArray{T,1} where T&lt;:Real; K::Int=4, tau::Int=1, Logx::Real=exp(1), Norm::Bool=true, Plotx::Bool=false)</code></pre><p>Returns the phase entropy (<code>Phas</code>) estimate of the data sequence (<code>Sig</code>)   using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>K</code>     - Angular partitions (coarse graining), an integer &gt; 1  </p><p><code>tau</code>   - Time Delay, a positive integer    </p><p><code>Logx</code>  - Logarithm base, a positive scalar </p><p><code>Norm</code>  - Normalisation of <code>Phas</code> value:    </p><pre><code class="language-none">      [false]  no normalisation
      [true]   normalises w.r.t. the number of partitions Log(`K`)</code></pre><p><code>Plotx</code> - When <code>Plotx</code> == true, returns Poicaré plot (default: false)  </p><p><strong>See also <code>SampEn</code>, <code>ApEn</code>, <code>GridEn</code>, <code>MSEn</code>, <code>SlopEn</code>, <code>CoSiEn</code>, <code>BubbEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Ashish Rohila and Ambalika Sharma,
    &quot;Phase entropy: a new complexity measure for heart rate
    variability.&quot; 
    Physiological measurement
    40.10 (2019): 105006.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_PhasEn.jl#L5-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._SlopEn.SlopEn" href="#EntropyHub._SlopEn.SlopEn"><code>EntropyHub._SlopEn.SlopEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Slop = SlopEn(Sig)</code></pre><p>Returns the slope entropy (<code>Slop</code>) estimates for embedding dimensions [2, ..., m] of the data sequence (<code>Sig</code>) using the default parameters: embedding dimension = 2, time delay = 1,  angular thresholds = [5 45],  logarithm = base 2 </p><pre><code class="language-none">Slop = SlopEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, Lvls::AbstractArray{T,1} where T&lt;:Real=[5, 45], Logx::Real=2, Norm::Bool=true)</code></pre><p>Returns the slope entropy (<code>Slop</code>) estimate of the data sequence (<code>Sig</code>)   using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, an integer &gt; 1   	</p><pre><code class="language-none">      SlopEn returns estimates for each dimension [2,...,m]</code></pre><p><code>tau</code>   - Time Delay, a positive integer    </p><p><code>Lvls</code>  - Angular thresolds, a vector of monotonically increasing              values in the range [0 90] degrees.</p><p><code>Logx</code>  - Logarithm base, a positive scalar (enter 0 for natural log)   </p><p><code>Norm</code>  - Normalisation of SlopEn value, a boolean operator: </p><pre><code class="language-none">      [false]  no normalisation
      [true]   normalises w.r.t. the number of patterns found (default)</code></pre><p><strong>See also <code>PhasEn</code>, <code>GridEn</code>, <code>MSEn</code>, <code>CoSiEn</code>, <code>SampEn</code>, <code>ApEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] David Cuesta-Frau,
    &quot;Slope Entropy: A New Time Series Complexity Estimator Based on
    Both Symbolic Patterns and Amplitude Information.&quot; 
    Entropy 
    21.12 (2019): 1167.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_SlopEn.jl#L4-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._BubbEn.BubbEn" href="#EntropyHub._BubbEn.BubbEn"><code>EntropyHub._BubbEn.BubbEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Bubb, H = BubbEn(Sig)</code></pre><p>Returns the bubble entropy (<code>Bubb</code>) and the conditional Rényi entropy (<code>H</code>) estimates of dimension m = 2 from the data sequence (<code>Sig</code>) using  the default parameters:  embedding dimension = 2, time delay = 1, logarithm = natural </p><pre><code class="language-none">Bubb, H = BubbEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=2, tau::Int=1, Logx::Real=exp(1))</code></pre><p>Returns the bubble entropy (<code>Bubb</code>) estimate of the data sequence (<code>Sig</code>)   using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Embedding Dimension, an integer &gt; 1   </p><pre><code class="language-none">      BubbEn returns estimates for each dimension [2,...,m]</code></pre><p><code>tau</code>   - Time Delay, a positive integer    </p><p><code>Logx</code>  - Logarithm base, a positive scalar </p><p><strong>See also <code>PhasEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] George Manis, M.D. Aktaruzzaman and Roberto Sassi,
    &quot;Bubble entropy: An entropy almost free of parameters.&quot;
    IEEE Transactions on Biomedical Engineering
    64.11 (2017): 2711-2718.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_BubbEn.jl#L4-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._GridEn.GridEn" href="#EntropyHub._GridEn.GridEn"><code>EntropyHub._GridEn.GridEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">GDE, GDR, _ = GridEn(Sig)</code></pre><p>Returns the gridded distribution entropy (<code>GDE</code>) and the gridded  distribution rate (<code>GDR</code>) estimated from the data sequence (<code>Sig</code>) using  the default  parameters: grid coarse-grain = 3, time delay = 1, logarithm = base 2</p><pre><code class="language-none">GDE, GDR, PIx, GIx, SIx, AIx = GridEn(Sig)</code></pre><p>In addition to GDE and GDR, GridEn returns the following indices  estimated for the data sequence (<code>Sig</code>) using the default  parameters: [<code>PIx</code>]   - Percentage of points below the line of identity (LI) [<code>GIx</code>]   - Proportion of point distances above the LI [<code>SIx</code>]   - Ratio of phase angles (w.r.t. LI) of the points above the LI [<code>AIx</code>]   - Ratio of the cumulative area of sectors of points above the LI</p><pre><code class="language-none">GDE, GDR, ..., = GridEn(Sig::AbstractArray{T,1} where T&lt;:Real; m::Int=3, tau::Int=1, Logx::Real=exp(1), Plotx::Bool=false)</code></pre><p>Returns the gridded distribution entropy (<code>GDE</code>) estimated from the data  sequence (<code>Sig</code>) using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>m</code>     - Grid coarse-grain (m x m sectors), an integer &gt; 1 </p><p><code>tau</code>   - Time Delay, a positive integer    </p><p><code>Logx</code>  - Logarithm base, a positive scalar </p><p><code>Plotx</code> - When Plotx == true, returns gridded Poicaré plot and a bivariate            histogram of the grid point distribution (default: false)  </p><p><strong>See also <code>PhasEn</code>, <code>CoSiEn</code>, <code>SlopEn</code>, <code>BubbEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Chang Yan, et al.,
        &quot;Novel gridded descriptors of poincaré plot for analyzing 
        heartbeat interval time-series.&quot; 
        Computers in biology and medicine 
        109 (2019): 280-289.

[2] Chang Yan, et al. 
        &quot;Area asymmetry of heart rate variability signal.&quot; 
        Biomedical engineering online 
        16.1 (2017): 1-14.

[3] Alberto Porta, et al.,
        &quot;Temporal asymmetries of short-term heart period variability 
        are linked to autonomic regulation.&quot; 
        American Journal of Physiology-Regulatory, Integrative and 
        Comparative Physiology 
        295.2 (2008): R550-R557.

[4] C.K. Karmakar, A.H. Khandoker and M. Palaniswami,
        &quot;Phase asymmetry of heart rate variability signal.&quot; 
        Physiological measurement 
        36.2 (2015): 303.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_GridEn.jl#L6-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._EnofEn.EnofEn" href="#EntropyHub._EnofEn.EnofEn"><code>EntropyHub._EnofEn.EnofEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">EoE, AvEn = EnofEn(Sig)</code></pre><p>Returns the entropy of entropy (<code>EoE</code>) and the average Shannon entropy  across all windows (<code>AvEn</code>) estimated from the data sequence (<code>Sig</code>) using the default parameters:  window length (samples) = 10, slices (s1,s2) = [10 5], logarithm = natural</p><pre><code class="language-none">EoE, AvEn = EnofEn(Sig::AbstractArray{T,1} where T&lt;:Real; tau::Int=10, S::Tuple{Int,Int}=(10,5), Logx::Real=exp(1))</code></pre><p>Returns the entropy of entropy (<code>EoE</code>) estimated from the data sequence  (<code>Sig</code>)  using the specified &#39;keyword&#39; arguments:</p><p><strong>Arguments:</strong></p><p><code>tau</code>   - Window length, an integer &gt; 1 </p><p><code>S</code>     - Number of slices (s1,s2), a two-element tuple of integers &gt; 2 </p><p><code>Logx</code>  - Logarithm base, a positive scalar  </p><p><strong>See also <code>SampEn</code>, <code>MSEn</code>, <code>ApEn</code></strong></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Chang Francis Hsu, et al.,
    &quot;Entropy of entropy: Measurement of dynamical complexity for
    biological systems.&quot; 
    Entropy 
    19.10 (2017): 550.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_EnofEn.jl#L4-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EntropyHub._AttnEn.AttnEn" href="#EntropyHub._AttnEn.AttnEn"><code>EntropyHub._AttnEn.AttnEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">Av4, (Hxx,Hnn,Hxn,Hnx) = AttnEn(Sig)</code></pre><p>Returns the attention entropy (<code>Av4</code>) calculated as the average of the sub-entropies (<code>Hxx</code>,<code>Hxn</code>,<code>Hnn</code>,<code>Hnx</code>) estimated from the data sequence (<code>Sig</code>) using a base-2 logarithm.</p><pre><code class="language-none">Av4, (Hxx, Hnn, Hxn, Hnx) = AttnEn(Sig::AbstractArray{T,1} where T&lt;:Real; Logx::Real=2)</code></pre><p>Returns the attention entropy (<code>Av4</code>) and the sub-entropies (<code>Hxx</code>,<code>Hnn</code>,<code>Hxn</code>,<code>Hnx</code>) from the data sequence (<code>Sig</code>) where, Hxx:    entropy of local-maxima intervals Hnn:    entropy of local minima intervals Hxn:    entropy of intervals between local maxima and subsequent minima Hnx:    entropy of intervals between local minima and subsequent maxima</p><p><strong>Arguments:</strong></p><p><code>Logx</code>  - Logarithm base, a positive scalar             (Enter 0 for natural logarithm)</p><p>See also <code>EnofEn</code>, <code>SpecEn</code>, <code>XSpecEn</code>, <code>PermEn</code>, <code>MSEn</code></p><p><strong>References:</strong></p><pre><code class="language-none">[1] Jiawei Yang, et al.,
    &quot;Classification of Interbeat Interval Time-series Using 
    Attention Entropy.&quot; 
    IEEE Transactions on Affective Computing 
    (2020)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/efa0c7cdddf0506738bf11b605ce8b811581721f/src/_AttnEn.jl#L4-L34">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../Cross_Entropies/">Cross-Entropies »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 23 May 2021 11:26">Sunday 23 May 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
