<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multivariate Entropies · EntropyHub.jl</title><meta name="title" content="Multivariate Entropies · EntropyHub.jl"/><meta property="og:title" content="Multivariate Entropies · EntropyHub.jl"/><meta property="twitter:title" content="Multivariate Entropies · EntropyHub.jl"/><meta name="description" content="Entropy of multivariate time series "/><meta property="og:description" content="Entropy of multivariate time series "/><meta property="twitter:description" content="Entropy of multivariate time series "/><meta property="og:url" content="https://mattwillflood.github.io/EntropyHub.jl/Guide/Multivariate_Entropies/"/><meta property="twitter:url" content="https://mattwillflood.github.io/EntropyHub.jl/Guide/Multivariate_Entropies/"/><link rel="canonical" href="https://mattwillflood.github.io/EntropyHub.jl/Guide/Multivariate_Entropies/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="EntropyHub.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">EntropyHub.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Base_Entropies/">Base Entropies</a></li><li><a class="tocitem" href="../Cross_Entropies/">Cross-Entropies</a></li><li class="is-active"><a class="tocitem" href>Multivariate Entropies</a></li><li><a class="tocitem" href="../Bidimensional_Entropies/">Bidimensional Entropies</a></li><li><a class="tocitem" href="../Multiscale_Entropies/">Multiscale Entropies</a></li><li><a class="tocitem" href="../Multiscale_Cross_Entropies/">Multiscale Cross-Entropies</a></li><li><a class="tocitem" href="../Multivariate_Multiscale_Entropies/">Multivariate Multiscale Entropies</a></li><li><a class="tocitem" href="../Other/">Other Functions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../Examples/Examples/">Notes on Examples</a></li><li><a class="tocitem" href="../../Examples/Example1/">Ex.1: Sample Entropy</a></li><li><a class="tocitem" href="../../Examples/Example2/">Ex.2: Permutation Entropy</a></li><li><a class="tocitem" href="../../Examples/Example3/">Ex.3: Phase Entropy</a></li><li><a class="tocitem" href="../../Examples/Example4/">Ex.4: Cross-Distribution Entropy</a></li><li><a class="tocitem" href="../../Examples/Example5/">Ex.5: Multiscale Entropy Object</a></li><li><a class="tocitem" href="../../Examples/Example6/">Ex.6: Multiscale [Increment] Entropy</a></li><li><a class="tocitem" href="../../Examples/Example7/">Ex.7: Refined Multiscale [Sample] Entropy</a></li><li><a class="tocitem" href="../../Examples/Example8/">Ex.8: Composite Multiscale Cross-Approximate Entropy</a></li><li><a class="tocitem" href="../../Examples/Example9/">Ex.9: Hierarchical Multiscale corrected Cross-Conditional Entropy</a></li><li><a class="tocitem" href="../../Examples/Example10/">Ex.10: Bidimensional Fuzzy Entropy</a></li><li><a class="tocitem" href="../../Examples/Example11/">Ex.11: Multivariate Dispersion Entropy</a></li><li><a class="tocitem" href="../../Examples/Example12/">Ex.12: [Generalized] Refined-composite Multivariate Multiscale Fuzzy Entropy</a></li><li><a class="tocitem" href="../../Examples/Example13/">Ex.13: Window Data Tool</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guide</a></li><li class="is-active"><a href>Multivariate Entropies</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multivariate Entropies</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MattWillFlood/EntropyHub.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/master/docs/src/Guide/Multivariate_Entropies.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Multivariate-Entropies"><a class="docs-heading-anchor" href="#Multivariate-Entropies">Multivariate Entropies</a><a id="Multivariate-Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Multivariate-Entropies" title="Permalink"></a></h1><p><strong><em>Functions for estimating the entropy of a multivariate time series dataset.</em></strong></p><p><code>The following functions also form the multivariate entropy method used by Multivariate Multiscale functions.</code> -&gt; <a href="../Multivariate_Multiscale_Entropies/#EntropyHub._MvMSEn.MvMSEn"><code>MvMSEn</code></a>, <a href="../Multivariate_Multiscale_Entropies/#EntropyHub._cMvMSEn.cMvMSEn"><code>cMvMSEn</code></a></p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="EntropyHub._MvSampEn.MvSampEn" href="#EntropyHub._MvSampEn.MvSampEn"><code>EntropyHub._MvSampEn.MvSampEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MSamp, B0, Bt, B1 = MvSampEn(Data)</code></pre><p>Returns the multivariate sample entropy estimate (<code>MSamp</code>) and the  average number of matched delay vectors (<code>m</code>: <code>B0</code>; joint total   <code>m+1</code> subspace: <code>Bt</code>; all possible <code>m+1</code> subspaces: <code>B1</code>),  from the M multivariate sequences in <code>Data</code> using the default parameters:   embedding dimension = 2*ones(M), time delay = ones(M), radius threshold = 0.2,  logarithm = natural, data normalization = false</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The entropy value returned as <code>MSamp</code> is estimated using the &quot;full&quot;  method [i.e.  -log(Bt/B0)] which compares delay vectors across all possible <code>m+1</code>  expansions of the embedding space as applied in [1][2]. Contrary to conventional definitions of sample entropy, this method does not provide a lower bound of 0!! Thus, it is possible to obtain negative entropy values for multivariate  sample entropy, even for stochastic processes...</p><p>Alternatively, one can calculate <code>MSamp</code> via the &quot;naive&quot; method,  which ensures a lower bound of 0, by using the average number of matched vectors for an individual <code>m+1</code> subspace (B1) [e.g. -log(B1(1)/B0)], or the average for all <code>m+1</code> subspaces [i.e. -log(mean(B1)/B0)].</p><p>To maximize the number of points in the embedding process, this algorithm  uses N - max(m * tau) delay vectors and <em><strong>not</strong></em> N-max(m) * max(tau) as employed  in [1][2].</p></div></div><hr/><pre><code class="nohighlight hljs">MSamp, B0, Bt, B1 = MvSampEn(Data::AbstractArray{T} where T&lt;:Real; m::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, tau::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, r::Real=0.2, Logx::Real=exp(1), Norm::Bool=false)</code></pre><p>Returns the multivariate sample entropy estimates (<code>MSamp</code>) estimated  from the M multivariate data sequences in <code>Data</code> using the specified   keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>Data</code>  - Multivariate dataset, NxM matrix of N (&gt;10) observations (rows) and M (cols) univariate data sequences</p><p><code>m</code>     - Embedding Dimension, a vector of M positive integers</p><p><code>tau</code>   - Time Delay, a vector of M positive integers</p><p><code>r</code>     - Radius Distance Threshold, a positive scalar  </p><p><code>Logx</code>  - Logarithm base, a positive scalar </p><p><code>Norm</code>  - Normalisation of all M sequences to unit variance, a boolean</p><p><strong>See also <code>SampEn</code>, <code>XSampEn</code>, <code>SampEn2D</code>, <code>MSEn</code>, <code>MvFuzzEn</code>, <code>MvPermEn</code></strong></p><p><strong>References:</strong></p><pre><code class="nohighlight hljs">[1] Ahmed Mosabber Uddin, Danilo P. Mandic
    &quot;Multivariate multiscale entropy: A tool for complexity
    analysis of multichannel data.&quot;
    Physical Review E 84.6 (2011): 061918.

[2] Ahmed Mosabber Uddin, Danilo P. Mandic
    &quot;Multivariate multiscale entropy analysis.&quot;
    IEEE signal processing letters 19.2 (2011): 91-94.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/d8372095a73672ad22477b87dec1c2123c3215eb/src/_MvSampEn.jl#L5-L68">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="EntropyHub._MvFuzzEn.MvFuzzEn" href="#EntropyHub._MvFuzzEn.MvFuzzEn"><code>EntropyHub._MvFuzzEn.MvFuzzEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MFuzz, B0, Bt, B1 = MvFuzzEn(Data)</code></pre><p>Returns the multivariate fuzzy entropy estimate (<code>MFuzz</code>) and the   average vector distances (<code>m</code>: <code>B0</code>; joint total <code>m+1</code> subspace: <code>Bt</code>;   all possible <code>m+1</code> subspaces: <code>B1</code>), from the M multivariate sequences  in <code>Data</code> using the default parameters:   embedding dimension = 2*ones(M,1), time delay = ones(M,1),   fuzzy membership function = &quot;default&quot;, fuzzy function parameters= [0.2, 2],  logarithm = natural, data normalization = false,</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The entropy value returned as <code>MFuzz</code> is estimated using the &quot;full&quot;  method [i.e.  -log(Bt/B0)] which compares delay vectors across all possible <code>m+1</code>  expansions of the embedding space as applied in [1][3]. Contrary to conventional definitions of sample entropy, this method does not provide a lower bound of 0!! Thus, it is possible to obtain negative entropy values for multivariate  fuzzy entropy, even for stochastic processes...</p><p>Alternatively, one can calculate <code>MFuzz</code> via the &quot;naive&quot; method,  which ensures a lower bound of 0, by using the average vector distances for an individual <code>m+1</code> subspace (B1) [e.g. -log(B1(1)/B0)], or the average for all <code>m+1</code> subspaces [i.e. -log(mean(B1)/B0)].</p><p>To maximize the number of points in the embedding process, this algorithm  uses N - max(m * tau) delay vectors and <em><em>not</em></em> N - max(m) * max(tau) as employed  in [1] and [3].</p></div></div><hr/><pre><code class="nohighlight hljs">MFuzz, B0, Bt, B1 = MvFuzzEn(Data::AbstractArray{T} where T&lt;:Real; m::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, tau::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, r::Union{Real,Tuple{Real,Real}}=(.2,2.0), Fx::String=&quot;default&quot;, Logx::Real=exp(1), Norm::Bool=false)</code></pre><p>Returns the multivariate sample entropy estimates (<code>MSamp</code>) estimated  from the M multivariate data sequences in <code>Data</code> using the specified   keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>Data</code>  - Multivariate dataset, NxM matrix of N (&gt;10) observations (rows) and M (cols) univariate data sequences</p><p><code>m</code>     - Embedding Dimension, a vector of M positive integers</p><p><code>tau</code>   - Time Delay, a vector of M positive integers</p><p><code>Fx</code>    - Fuzzy function name, one of the following:              {<code>&quot;sigmoid&quot;, &quot;modsampen&quot;, &quot;default&quot;, &quot;gudermannian&quot;,</code>             <code>&quot;bell&quot;, &quot;triangular&quot;, &quot;trapezoidal1&quot;, &quot;trapezoidal2&quot;,</code>             <code>&quot;z_shaped&quot;, &quot;gaussian&quot;, &quot;constgaussian&quot;</code>}</p><p><code>r</code>      - Fuzzy function parameters, a 1 element scalar or a 2 element                 tuple of positive values. The <code>r</code> parameters for each fuzzy                 function are defined as follows:      [default: [.2 2]]</p><pre><code class="nohighlight hljs">            default:        r(1) = divisor of the exponential argument
                            r(2) = argument exponent (pre-division)
            sigmoid:        r(1) = divisor of the exponential argument
                            r(2) = value subtracted from argument (pre-division)
            modsampen:      r(1) = divisor of the exponential argument
                            r(2) = value subtracted from argument (pre-division)
            gudermannian:   r  = a scalar whose value is the numerator of
                                argument to gudermannian function:
                                GD(x) = atan(tanh(`r`/x))
            triangular:     r = a scalar whose value is the threshold (corner point) of the triangular function.
            trapezoidal1:   r = a scalar whose value corresponds to the upper (2r) and lower (r) corner points of the trapezoid.
            trapezoidal2:   r(1) = a value corresponding to the upper corner point of the trapezoid.
                            r(2) = a value corresponding to the lower corner point of the trapezoid.
            z_shaped:       r = a scalar whose value corresponds to the upper (2r) and lower (r) corner points of the z-shape.
            bell:           r(1) = divisor of the distance value
                            r(2) = exponent of generalized bell-shaped function
            gaussian:       r = a scalar whose value scales the slope of the Gaussian curve.
            constgaussian:  r = a scalar whose value defines the lower threshod and shape of the Gaussian curve.</code></pre><p><code>Logx</code>   - Logarithm base, a positive scalar </p><p><code>Norm</code>   - Normalisation of all M sequences to unit variance, a boolean</p><p><strong>See also <code>MvSampEn</code>, <code>FuzzEn</code>, <code>XFuzzEn</code>, <code>FuzzEn2D</code>, <code>MSEn</code>, <code>MvPermEn</code></strong></p><p><strong>References:</strong></p><pre><code class="nohighlight hljs">[1] Ahmed, Mosabber U., et al. 
    &quot;A multivariate multiscale fuzzy entropy algorithm with application
    to uterine EMG complexity analysis.&quot; 
    Entropy 19.1 (2016): 2.

[2] Azami, Alberto Fernández, Javier Escudero. 
    &quot;Refined multiscale fuzzy entropy based on standard deviation for 
    biomedical signal analysis.&quot; 
    Medical &amp; biological engineering &amp; computing 55 (2017): 2037-2052.

[3] Ahmed Mosabber Uddin, Danilo P. Mandic
    &quot;Multivariate multiscale entropy analysis.&quot;
    IEEE signal processing letters 19.2 (2011): 91-94.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/d8372095a73672ad22477b87dec1c2123c3215eb/src/_MvFuzzEn.jl#L4-L101">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="EntropyHub._MvPermEn.MvPermEn" href="#EntropyHub._MvPermEn.MvPermEn"><code>EntropyHub._MvPermEn.MvPermEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MPerm, MPnorm = MvPermEn(Data)</code></pre><p>Returns the multivariate permutation entropy estimate (<code>MPerm</code>) and  the normalized permutation entropy for the M multivariate sequences in  <code>Data</code> using the default parameters:  embedding dimension = 2*ones(M,1), time delay = ones(M,1),   logarithm = 2, normalisation = w.r.t #symbols (sum(<code>m-1</code>))</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The multivariate permutation entropy algorithm implemented here uses multivariate embedding based on Takens&#39; embedding theorem, and follows the methods for multivariate entropy estimation through shared spatial  reconstruction as originally presented by Ahmed &amp; Mandic [1]. </p><p>This function does <em><em>NOT</em></em> use the multivariate permutation entropy  algorithm of Morabito et al. (Entropy, 2012) where the entropy values of  individual univariate sequences are averaged because such methods do not follow the definition of multivariate embedding and therefore do not consider cross-channel statistical complexity.</p><p>To maximize the number of points in the embedding process, this algorithm uses N- max(tau * m) delay vectors and <em><em>not</em></em> N-max(m) * max(tau) as employed in [1].</p></div></div><hr/><pre><code class="nohighlight hljs">MPerm, MPnorm = MvPermEn(Data::AbstractArray{T} where T&lt;:Real; m::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, tau::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, Typex::String=&quot;none&quot;, tpx::Union{Int,Nothing}=nothing, Norm::Bool=false, Logx::Real=2)</code></pre><p>Returns the multivariate permutation entropy estimate (<code>MPerm</code>) for  the M multivariate data sequences in <code>Data</code> using the specified keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>Data</code>  - Multivariate dataset, NxM matrix of N (&gt;10) observations (rows) and M (cols) univariate data sequences</p><p><code>m</code>     - Embedding Dimension, a vector of M positive integers</p><p><code>tau</code>   - Time Delay, a vector of M positive integers</p><p><code>Typex</code> - Permutation entropy variation, can be one of the following strings:</p><pre><code class="nohighlight hljs">        {`&#39;modified&#39;`, `&#39;ampaware&#39;`, `&#39;weighted&#39;`, `&#39;edge&#39;`, `&#39;phase&#39;`}
        See the `EntropyHub guide &lt;https://github.com/MattWillFlood/EntropyHub/blob/main/EntropyHub%20Guide.pdf&gt;`_ for more info on MvPermEn variants.</code></pre><p><code>tpx</code>   - Tuning parameter for associated permutation entropy variation. </p><pre><code class="nohighlight hljs">        *   [ampaware]  `tpx` is the A parameter, a value in range [0 1]; default = 0.5
        *   [edge]      `tpx` is the r sensitivity parameter, a scalar &gt; 0; default = 1
        *   [phase]     `tpx` is the option to unwrap the phase angle of Hilbert-transformed signal, either [] or 1 (default = 0)</code></pre><p><code>Norm</code>  - Normalisation of MPnorm value, a boolean operator:</p><pre><code class="nohighlight hljs">        *   false -  normalises w.r.t log(# of permutation symbols [sum(m)-1]) - default
        *   true  -  normalises w.r.t log(# of all possible permutations [sum(m)!])</code></pre><p><code>Logx</code>   - Logarithm base, a positive scalar </p><p><strong>See also    <code>PermEn</code>, <code>PermEn2D</code>, <code>XPermEn</code>, <code>MSEn</code>, <code>MvFuzzEn</code>, <code>MvSampEn</code></strong></p><p><strong>References:</strong></p><pre><code class="nohighlight hljs">[1] Ahmed Mosabber Uddin, Danilo P. Mandic
    &quot;Multivariate multiscale entropy: A tool for complexity
    analysis of multichannel data.&quot;
    Physical Review E 84.6 (2011): 061918.

[2] Christoph Bandt and Bernd Pompe, 
    &quot;Permutation entropy: A natural complexity measure for time series.&quot; 
    Physical Review Letters,
    88.17 (2002): 174102.

[3] Chunhua Bian, et al.,
    &quot;Modified permutation-entropy analysis of heartbeat dynamics.&quot;
    Physical Review E
    85.2 (2012) : 021906

[4] Bilal Fadlallah, et al.,
    &quot;Weighted-permutation entropy: A complexity measure for time 
    series incorporating amplitude information.&quot; 
    Physical Review E 
    87.2 (2013): 022911.

[5] Hamed Azami and Javier Escudero,
    &quot;Amplitude-aware permutation entropy: Illustration in spike 
    detection and signal segmentation.&quot; 
    Computer methods and programs in biomedicine,
    128 (2016): 40-51.

[6] Zhiqiang Huo, et al.,
    &quot;Edge Permutation Entropy: An Improved Entropy Measure for 
    Time-Series Analysis,&quot; 
    45th Annual Conference of the IEEE Industrial Electronics Soc,
    (2019), 5998-6003

[7] Maik Riedl, Andreas Müller, and Niels Wessel,
    &quot;Practical considerations of permutation entropy.&quot; 
    The European Physical Journal Special Topics 
    222.2 (2013): 249-262.

[8] Kang Huan, Xiaofeng Zhang, and Guangbin Zhang,
    &quot;Phase permutation entropy: A complexity measure for nonlinear time
    series incorporating phase information.&quot;
    Physica A: Statistical Mechanics and its Applications
    568 (2021): 125686.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/d8372095a73672ad22477b87dec1c2123c3215eb/src/_MvPermEn.jl#L6-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="EntropyHub._MvDispEn.MvDispEn" href="#EntropyHub._MvDispEn.MvDispEn"><code>EntropyHub._MvDispEn.MvDispEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MDisp, RDE = MvDispEn(Data)</code></pre><p>Returns the multivariate dispersion entropy estimate (<code>MDisp</code>) and  the reverse dispersion entropy (<code>RDE</code>) for the M multivariate sequences   in <code>Data</code> using the default parameters:  embedding dimension = 2*ones(M,1), time delay = ones(M,1), # symbols = 3,   algorithm method = &quot;v1&quot; (see below), data transform = normalised cumulative density function (ncdf)  logarithm = natural, data normalization = true,</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>By default, <code>MvDispEn</code> uses the method termed <code>mvDEii</code> in [1], which follows the original multivariate embedding approach of Ahmed &amp; Mandic [2]. The <code>v1</code> method therefore returns a singular entropy estimate.</p><p>If the <code>v2</code> method is selected (<code>Methodx==&quot;v2&quot;</code>), the main method outlined in [1] termed <code>mvDE</code> is applied. In this case, entropy is estimated using each combination of multivariate delay vectors with lengths 1:max(m), with each entropy value returned accordingly. See [1] for more info.</p></div></div><hr/><pre><code class="nohighlight hljs">MDisp, RDE = MvDispEn(Data::AbstractArray{T,2} where T&lt;:Real; m::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, tau::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, c::Int=3, Methodx::String=&quot;v1&quot;, Typex::String=&quot;NCDF&quot;, Norm::Bool=false, Logx::Real=exp(1))</code></pre><p>Returns the multivariate dispersion entropy estimate (<code>MDisp</code>) for the M  multivariate data sequences in <code>Data</code> using the specified keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>Data</code>    - Multivariate dataset, NxM matrix of N (&gt;10) observations (rows) and M (cols) univariate data sequences</p><p><code>m</code>       - Embedding Dimension, a vector of M positive integers</p><p><code>tau</code>     - Time Delay, a vector of M positive integers</p><p><code>c</code>       - Number of symbols in transform, an integer &gt; 1 </p><p><code>Methodx</code> - The method of multivariate dispersion entropy estimation as outlined in [1], either:</p><pre><code class="nohighlight hljs">    * `&quot;v1&quot;` - employs the method consistent with the original multivariate embedding approach of Ahmed &amp;
                Mandic [2], termed `mvDEii` in [1]. (default)
    * `&quot;v2&quot;` - employs the main method derived in [1],  termed `mvDE`.</code></pre><p><code>Typex</code>   - Type of data-to-symbolic sequence transform, one of the following:</p><pre><code class="nohighlight hljs">            {`&#39;linear&#39;`, `&#39;kmeans&#39;`, `&#39;ncdf&#39;`, `&#39;equal&#39;`}
        See the `EntropyHub Guide` for more info on these transforms.</code></pre><p><code>Norm</code>    - Normalisation of <code>MDisp</code> and <code>RDE</code> values, a boolean:</p><pre><code class="nohighlight hljs">            * [false]   no normalisation (default)
            * [true]    normalises w.r.t number of possible dispersion patterns (`c^m`).</code></pre><p><code>Logx</code>   - Logarithm base, a positive scalar </p><p><strong>See also   <code>DispEn</code>, <code>DispEn2D</code>, <code>MvSampEn</code>, <code>MvFuzzEn</code>, <code>MvPermEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="nohighlight hljs">[1] H Azami, A Fernández, J Escudero
      &quot;Multivariate Multiscale Dispersion Entropy of Biomedical Times Series&quot;
      Entropy 2019, 21, 913.

[2] Ahmed Mosabber Uddin, Danilo P. Mandic
      &quot;Multivariate multiscale entropy: A tool for complexity
      analysis of multichannel data.&quot;
      Physical Review E 84.6 (2011): 061918.

[3] Mostafa Rostaghi and Hamed Azami,
       &quot;Dispersion entropy: A measure for time-series analysis.&quot; 
       IEEE Signal Processing Letters 
       23.5 (2016): 610-614.

[4] Hamed Azami and Javier Escudero,
       &quot;Amplitude-and fluctuation-based dispersion entropy.&quot; 
       Entropy 
       20.3 (2018): 210.

[5] Li Yuxing, Xiang Gao and Long Wang,
       &quot;Reverse dispersion entropy: A new complexity measure for sensor signal.&quot; 
       Sensors 
       19.23 (2019): 5203.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/d8372095a73672ad22477b87dec1c2123c3215eb/src/_MvDispEn.jl#L7-L88">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="EntropyHub._MvCoSiEn.MvCoSiEn" href="#EntropyHub._MvCoSiEn.MvCoSiEn"><code>EntropyHub._MvCoSiEn.MvCoSiEn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MCoSi, Bm = MvCoSiEn(Data)</code></pre><p>Returns the multivariate cosine similarity entropy estimate (<code>MCoSi</code>)  and the corresponding global probabilities (<code>Bm</code>) estimated for the   M multivariate sequences in <code>Data</code> using the default parameters:   embedding dimension = 2*ones(M), time delay = ones(M),   angular threshold = 0.1, logarithm = 2, data normalization = none, </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>To maximize the number of points in the embedding process, this algorithm  uses N-max(m * tau) delay vectors and <em><em>not</em></em> N-max(m) * max(tau) as employed  in [1][2].</p></div></div><hr/><pre><code class="nohighlight hljs">MCoSi, Bm = MvCoSiEn(Data::AbstractArray{T,2} where T&lt;:Real; m::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, tau::Union{AbstractArray{T} where T&lt;:Int, Nothing}=nothing, r::Real=.1, Logx::Real=2, Norm::Int=0)</code></pre><p>Returns the multivariate cosine similarity entropy estimates (<code>MSamp</code>)   estimated from the M multivariate data sequences in <code>Data</code> using the   specified keyword arguments:</p><p><strong>Arguments:</strong></p><p><code>Data</code>    - Multivariate dataset, NxM matrix of N (&gt;10) observations (rows) and M (cols) univariate data sequences</p><p><code>m</code>       - Embedding Dimension, a vector of M positive integers</p><p><code>tau</code>     - Time Delay, a vector of M positive integers</p><p><code>r</code>     - Angular threshold, a value in range [0 &lt; r &lt; 1]   </p><p><code>Logx</code>    - Logarithm base, a positive scalar (enter 0 for natural log) </p><p><code>Norm</code>    - Normalisation of <code>Data</code>, one of the following integers:</p><pre><code class="nohighlight hljs">        *  [0]  no normalisation - default
        *  [1]  remove median(`Data`) to get zero-median series
        *  [2]  remove mean(`Data`) to get zero-mean series
        *  [3]  normalises each sequence in `Data` to unit variance and zero mean
        *  [4]  normalises each sequence in `Data` values to range [-1 1]</code></pre><p><strong>See also  <code>CoSiEn</code>, <code>MvDispEn</code>, <code>MvSampEn</code>, <code>MvFuzzEn</code>, <code>MvPermEn</code>, <code>MSEn</code></strong></p><p><strong>References:</strong></p><pre><code class="nohighlight hljs">[1] H. Xiao, T. Chanwimalueang and D. P. Mandic, 
    &quot;Multivariate Multiscale Cosine Similarity Entropy&quot; 
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pp. 5997-6001, doi: 10.1109/ICASSP43922.2022.9747282.

[2] Xiao, H.; Chanwimalueang, T.; Mandic, D.P., 
    &quot;Multivariate Multiscale Cosine Similarity Entropy and Its 
    Application to Examine Circularity Properties in Division Algebras.&quot;
    Entropy 2022, 24, 1287. 

[3] Ahmed Mosabber Uddin, Danilo P. Mandic
    &quot;Multivariate multiscale entropy: A tool for complexity
    analysis of multichannel data.&quot;
    Physical Review E 84.6 (2011): 061918.

[4] Theerasak Chanwimalueang and Danilo Mandic,
    &quot;Cosine similarity entropy: Self-correlation-based complexity
    analysis of dynamical systems.&quot;
    Entropy 
    19.12 (2017): 652.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MattWillFlood/EntropyHub.jl/blob/d8372095a73672ad22477b87dec1c2123c3215eb/src/_MvCoSiEn.jl#L5-L71">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Cross_Entropies/">« Cross-Entropies</a><a class="docs-footer-nextpage" href="../Bidimensional_Entropies/">Bidimensional Entropies »</a><div class="flexbox-break"></div><p class="footer-message"><a href="https://www.EntropyHub.xyz">www.EntropyHub.xyz</a> </p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 26 April 2024 13:58">Friday 26 April 2024</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
